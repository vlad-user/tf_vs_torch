{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import dataset\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import pickle\n",
    "\n",
    "train_data_size = 4000\n",
    "batch_size = 50\n",
    "DEVICE = 1\n",
    "n_epochs = 300\n",
    "learning_rate = 0.01\n",
    "dropout_rate = 0.4\n",
    "\n",
    "def compute_error(y_pred, y):\n",
    "    result = [1.0 if y1==y2 else 0.0\n",
    "              for y1, y2 in zip(y_pred.to('cpu'), y.type(torch.LongTensor).to('cpu'))]\n",
    "    return 1.0 - sum(result)/len(result)\n",
    "\n",
    "def _test_loss_err(model, loss_fn, loader):\n",
    "    errs, losses = [], []\n",
    "    for (x, y) in loader:\n",
    "        x, y = x.type(torch.FloatTensor), y.type(torch.FloatTensor)\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        y_pred = model(x)\n",
    "        loss_val = loss_fn(y_pred.type(torch.FloatTensor), y.type(torch.LongTensor))\n",
    "        vals, y_pred = torch.max(y_pred, 1)\n",
    "        error = compute_error(y_pred, y)\n",
    "        losses.append(loss_val.item())\n",
    "        errs.append(error)\n",
    "\n",
    "    loss = np.mean(losses)\n",
    "    error = np.mean(errs)\n",
    "\n",
    "    return loss, error\n",
    "\n",
    "class Dataset(dataset.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index], self.labels[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "def print_log(dict_):\n",
    "    buff = '|'.join(['['+str(k)+ ':' +\"{0:.3f}\".format(v)+']' for k, v in sorted(dict_.items())])\n",
    "    sys.stdout.write('\\r' + buff)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "class TorchModel(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.0, init=None):\n",
    "        super(TorchModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,\n",
    "                      out_channels=6,\n",
    "                      kernel_size=5,\n",
    "                      padding=0,\n",
    "                      bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(p=dropout_rate))\n",
    "        if init:\n",
    "            conv1_init = init['conv1']\n",
    "            self.conv1[0].weight = nn.Parameter(torch.FloatTensor(conv1_init))\n",
    "        else:\n",
    "            torch.nn.init.xavier_uniform_(self.conv1[0].weight)\n",
    "        torch.nn.init.zeros_(self.conv1[0].bias)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=6,\n",
    "                      out_channels=12,\n",
    "                      kernel_size=3,\n",
    "                      bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(p=dropout_rate))\n",
    "        if init:\n",
    "            conv2_init = init['conv2']\n",
    "            self.conv2[0].weight = nn.Parameter(torch.FloatTensor(conv2_init))\n",
    "        else:\n",
    "            torch.nn.init.xavier_uniform_(self.conv2[0].weight)\n",
    "        torch.nn.init.zeros_(self.conv2[0].bias)\n",
    "        \n",
    "        self.logits = nn.Linear(432, 10)\n",
    "        if init:\n",
    "            logits_init = init['logits']\n",
    "            logits_init = np.reshape(logits_init, [10, 432])\n",
    "            self.logits.weight = nn.Parameter(torch.FloatTensor(logits_init))\n",
    "        else:\n",
    "            torch.nn.init.xavier_uniform_(self.logits.weight)\n",
    "        torch.nn.init.zeros_(self.logits.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "    \n",
    "    def count_params(self):\n",
    "         return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "with open('init.pkl', 'rb') as fo:\n",
    "    init = pickle.load(fo)\n",
    "    \n",
    "with open('data.pkl', 'rb') as fo:\n",
    "    data = pickle.load(fo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:299.000]|[err:0.654]\n",
      "time_took 110.5572509765625\n",
      "min_error 0.6471000000000001\n"
     ]
    }
   ],
   "source": [
    "x_train = data['x_train']\n",
    "x_test = data['x_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "x_train, x_test = (np.reshape(x_train, [x_train.shape[0], 3, 32, 32]),\n",
    "                   np.reshape(x_test, [x_test.shape[0], 3, 32, 32]))\n",
    "min_errs = []\n",
    "time_took = []\n",
    "for i in range(1):\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(Dataset(x_train, y_train),\n",
    "                                               batch_size=batch_size)\n",
    "    test_loader = torch.utils.data.DataLoader(Dataset(x_test, y_test),\n",
    "                                              batch_size=batch_size)\n",
    "    start_time = time()\n",
    "    model = TorchModel(dropout_rate=dropout_rate)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "    loss_fn = loss_fn.to(DEVICE)\n",
    "\n",
    "    train_log = {'train_loss':[],\n",
    "                 'test_loss':[],\n",
    "                 'train_error':[],\n",
    "                 'test_error':[]}\n",
    "\n",
    "    model.eval()\n",
    "    loss_val, err_val = _test_loss_err(model, loss_fn, test_loader)\n",
    "    train_log['test_loss'].append(loss_val.item())\n",
    "    train_log['test_error'].append(err_val)\n",
    "    loss_val, err_val = _test_loss_err(model, loss_fn, train_loader)\n",
    "    train_log['train_loss'].append(loss_val.item())\n",
    "    train_log['train_error'].append(err_val)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        print_log({'epoch':epoch, 'err':train_log['test_error'][-1]})\n",
    "        for (x, y) in train_loader:\n",
    "            x, y = x.type(torch.FloatTensor).to(DEVICE), y.type(torch.FloatTensor).to(DEVICE)\n",
    "            y_pred = model(x)\n",
    "            loss_val = loss_fn(y_pred.type(torch.FloatTensor), y.type(torch.LongTensor))\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        loss_val, err_val = _test_loss_err(model, loss_fn, test_loader)\n",
    "        train_log['test_loss'].append(loss_val.item())\n",
    "        train_log['test_error'].append(err_val)\n",
    "        loss_val, err_val = _test_loss_err(model, loss_fn, train_loader)\n",
    "        train_log['train_loss'].append(loss_val.item())\n",
    "        train_log['train_error'].append(err_val)\n",
    "    with open('torch_log.pkl', 'wb') as fo:\n",
    "        train_log['n_epochs'] = n_epochs\n",
    "        pickle.dump(train_log, fo, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    time_took.append(time() - start_time)\n",
    "    min_errs.append(min(train_log['test_error']))\n",
    "print()\n",
    "print('time_took', np.mean(time_took))\n",
    "print('min_error', np.mean(min_errs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
