{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from time import time\n",
    "import pickle\n",
    "\n",
    "def generate_initializers_and_data():\n",
    "    \"\"\"**Note**: this function must be called to generate weights and data for \n",
    "    both jupyter files!!!\"\"\"\n",
    "    conv1_kernel = np.float32(np.random.normal(scale=0.5, size=[6, 3, 5, 5]))\n",
    "    conv2_kernel = np.float32(np.random.normal(scale=0.5, size=[12, 6, 3, 3]))\n",
    "    logits_kernel = np.float32(np.random.normal(scale=0.5, size=[432, 10]))\n",
    "    with open('init.pkl', 'wb') as fo:\n",
    "        init = {'conv1': conv1_kernel,\n",
    "                'conv2': conv2_kernel,\n",
    "                'logits': logits_kernel}\n",
    "        pickle.dump(init, fo, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    x_train, x_test = np.float32(x_train/255.0), np.float32(x_test/255.0)\n",
    "    y_train, y_test = np.int32(y_train), np.int32(y_test)\n",
    "    x_train, y_train = shuffle(x_train, y_train)\n",
    "    x_train, y_train = x_train[:train_data_size], y_train[:train_data_size]\n",
    "    y_train, y_test = y_train.flatten(), y_test.flatten()\n",
    "    data = {\n",
    "        'x_train':x_train,\n",
    "        'y_train':y_train,\n",
    "        'x_test':x_test,\n",
    "        'y_test':y_test\n",
    "    }\n",
    "    with open('data.pkl', 'wb') as fo:\n",
    "        pickle.dump(data, fo, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_epochs = 300\n",
    "batch_size = 50\n",
    "train_data_size = 4000\n",
    "dropout_rate = 0.4\n",
    "\n",
    "\n",
    "def print_log(dict_):\n",
    "    buff = '|'.join(['['+str(k)+ ':' +\"{0:.3f}\".format(v)+']' for k, v in sorted(dict_.items())])\n",
    "    sys.stdout.write('\\r' + buff)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def _test_loss_err(sess, loss_op, err_op, next_batch, iterator, x, y):\n",
    "    batch_loss = []\n",
    "    batch_err = []\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            batch = sess.run(next_batch)\n",
    "            loss, err = sess.run([loss_op, err_op], feed_dict={x:batch['x'], y:batch['y']})\n",
    "            batch_loss.append(loss)\n",
    "            batch_err.append(err)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            loss = np.mean(batch_loss)\n",
    "            err = np.mean(batch_err)\n",
    "            sess.run(iterator.initializer)\n",
    "            break\n",
    "    return loss, err\n",
    "\n",
    "def tf_model(graph, init=None):\n",
    "    with graph.as_default():\n",
    "        if init:\n",
    "            conv1_init = init['conv1']\n",
    "            conv2_init = init['conv2']\n",
    "            logits_init = init['logits']\n",
    "            conv1_init = tf.constant_initializer(conv1_init)\n",
    "            conv2_init = tf.constant_initializer(conv2_init)\n",
    "            logits_init = tf.constant_initializer(logits_init)\n",
    "        else:\n",
    "            conv1_init = tf.contrib.layers.xavier_initializer()\n",
    "            conv2_init = tf.contrib.layers.xavier_initializer()\n",
    "            logits_init = tf.contrib.layers.xavier_initializer()\n",
    "        with tf.name_scope('Input'):\n",
    "            \n",
    "            x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3], name='x')\n",
    "            y = tf.placeholder(tf.int32, shape=[None], name='y')\n",
    "            keep_prob = tf.placeholder_with_default(1.0 - dropout_rate, shape=())\n",
    "        with tf.device('/device:GPU:0'):\n",
    "            with tf.name_scope('conv1'):\n",
    "                conv1 = tf.layers.conv2d(x,\n",
    "                                         filters=6,\n",
    "                                         kernel_size=5,\n",
    "                                         strides=1,\n",
    "                                         padding='valid',\n",
    "                                         kernel_initializer=conv1_init,\n",
    "                                         bias_initializer=tf.initializers.zeros,\n",
    "                                         activation=tf.nn.relu,\n",
    "                                         name='conv1'\n",
    "                                         )\n",
    "\n",
    "                \n",
    "                max_pool1 = tf.nn.max_pool(value=conv1,\n",
    "                                           ksize=(1, 2, 2, 1),\n",
    "                                           strides=(1, 2, 2, 1),\n",
    "                                           padding='SAME',\n",
    "                                           name='max_pool1')\n",
    "\n",
    "                dropout1 = tf.nn.dropout(max_pool1, keep_prob=keep_prob)\n",
    "\n",
    "            with tf.name_scope('conv2'):\n",
    "                conv2 = tf.layers.conv2d(dropout1,\n",
    "                                         filters=12,\n",
    "                                         kernel_size=3,\n",
    "                                         strides=1,\n",
    "                                         padding='valid',\n",
    "                                         bias_initializer=tf.initializers.zeros,\n",
    "                                         activation=tf.nn.relu,\n",
    "                                         kernel_initializer=conv2_init,\n",
    "                                         name='conv2')\n",
    "\n",
    "                max_pool2 = tf.nn.max_pool(value=conv2,\n",
    "                                           ksize=(1, 2, 2, 1),\n",
    "                                           strides=(1, 2, 2, 1),\n",
    "                                           padding='VALID',\n",
    "                                           name='max_pool2')\n",
    "\n",
    "                dropout2 = tf.nn.dropout(max_pool2, keep_prob=keep_prob)\n",
    "\n",
    "            with tf.name_scope('logits'):\n",
    "                flatten = tf.layers.Flatten()(max_pool2)\n",
    "                logits = tf.layers.dense(flatten,\n",
    "                                         units=10,\n",
    "                                         kernel_initializer=logits_init,\n",
    "                                         bias_initializer=tf.initializers.zeros,\n",
    "                                         name='logits')\n",
    "\n",
    "    return x, y, keep_prob, logits\n",
    "# call `generate_initializers_and_data()` only once to generate data\n",
    "generate_initializers_and_data()\n",
    "with open('init.pkl', 'rb') as fo:\n",
    "    init = pickle.load(fo)\n",
    "\n",
    "with open('data.pkl', 'rb') as fo:\n",
    "    data = pickle.load(fo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:299.000]|[error:0.536]|[loss:1.572]\n",
      "time_took 290.59439420700073\n",
      "min_error 0.53139997\n"
     ]
    }
   ],
   "source": [
    "x_train = data['x_train']\n",
    "x_test = data['x_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "min_errs = []\n",
    "time_took = []\n",
    "for i in range(1):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    start_time = time()\n",
    "    x, y, keep_prob, logits = tf_model(tf.get_default_graph())\n",
    "    with tf.device('cpu'):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=y, logits=logits)\n",
    "        loss_op = tf.reduce_mean(xentropy)\n",
    "        y_pred = tf.nn.in_top_k(predictions=tf.cast(logits, tf.float32),\n",
    "                              targets=y,\n",
    "                              k=1)\n",
    "        err_op = 1.0 - tf.reduce_mean(tf.cast(x=y_pred, dtype=tf.float32),\n",
    "                                      name='error')\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(loss_op)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "    data = tf.data.Dataset.from_tensor_slices({'x':x_train, 'y':y_train}).batch(batch_size)\n",
    "    iter_train = data.make_initializable_iterator()\n",
    "    data = tf.data.Dataset.from_tensor_slices({'x':x_test, 'y':y_test}).batch(batch_size)\n",
    "    iter_test = data.make_initializable_iterator()\n",
    "\n",
    "    tf_log = {'train_loss':[],\n",
    "              'test_loss':[],\n",
    "              'train_error':[],\n",
    "              'test_error':[]\n",
    "             }\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        _ = sess.run([tf.global_variables_initializer(),\n",
    "                      iter_train.initializer,\n",
    "                      iter_test.initializer])\n",
    "        next_train = iter_train.get_next()\n",
    "        next_test = iter_test.get_next()\n",
    "\n",
    "        loss_val, err_val = _test_loss_err(sess, loss_op, err_op, next_test, iter_test, x, y)\n",
    "        tf_log['test_loss'].append(loss_val)\n",
    "        tf_log['test_error'].append(err_val)\n",
    "        loss_val, err_val = _test_loss_err(sess, loss_op, err_op, next_train, iter_train, x, y)\n",
    "        tf_log['train_loss'].append(loss_val)\n",
    "        tf_log['train_error'].append(err_val)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            while True:\n",
    "                try:\n",
    "                    batch = sess.run(next_train)\n",
    "                    _ = sess.run([train_op],\n",
    "                                 feed_dict={x:batch['x'], y:batch['y']})\n",
    "\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    sess.run(iter_train.initializer)\n",
    "                    loss_val, err_val = _test_loss_err(sess, loss_op, err_op, next_test, iter_test, x, y)\n",
    "                    tf_log['test_loss'].append(loss_val)\n",
    "                    tf_log['test_error'].append(err_val)\n",
    "                    print_log({'epoch':epoch, 'error':err_val, 'loss':loss_val})\n",
    "                    loss_val, err_val = _test_loss_err(sess, loss_op, err_op, next_train, iter_train, x, y)\n",
    "                    tf_log['train_loss'].append(loss_val)\n",
    "                    tf_log['train_error'].append(err_val)\n",
    "\n",
    "                    break\n",
    "    with open('tf_log.pkl', 'wb') as fo:\n",
    "        tf_log['n_epochs'] = n_epochs\n",
    "        pickle.dump(tf_log, fo, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    time_took.append(time() - start_time)\n",
    "    min_errs.append(min(tf_log['test_error']))\n",
    "print()\n",
    "print('time_took', np.mean(time_took))\n",
    "print('min_error', np.mean(min_errs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
